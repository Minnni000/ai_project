import os
import sys
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity


def analyze_recipe_similarity(input_csv_path, output_csv_path, threshold=0.5):
    """
    åˆ†æé£Ÿè­œç›¸ä¼¼åº¦ä¸¦è¼¸å‡ºçµæœåˆ° CSV æª”æ¡ˆ
    """
    # è®€å– CSV
    df = pd.read_csv(input_csv_path)

    if len(df) < 2:
        print(f"âš ï¸  æª”æ¡ˆ {input_csv_path} ä¸­çš„é£Ÿè­œæ•¸é‡å°‘æ–¼ 2 ç­†ï¼Œç„¡æ³•é€²è¡Œç›¸ä¼¼åº¦åˆ†æ")
        return

    # TF-IDF å‘é‡åŒ–
    vectorizer = TfidfVectorizer()
    tfidf_matrix = vectorizer.fit_transform(df["document"])

    # è¨ˆç®—ç›¸ä¼¼åº¦çŸ©é™£ï¼ˆNxNï¼‰
    cosine_sim = cosine_similarity(tfidf_matrix)

    print(f"ğŸ“Š åˆ†ææ‰€æœ‰é£Ÿè­œçµ„åˆçš„ç›¸ä¼¼åº¦...")

    # æ”¶é›†æ‰€æœ‰ç›¸ä¼¼åº¦æ•¸å€¼ï¼ˆåŒ…å«ä½æ–¼é–€æª»çš„ï¼‰
    all_similarities = []
    for i in range(len(df)):
        for j in range(i+1, len(df)):
            sim_score = cosine_sim[i][j]
            id1 = df.iloc[i]["id"]
            name1 = df.iloc[i]["name"]
            id2 = df.iloc[j]["id"]
            name2 = df.iloc[j]["name"]

            all_similarities.append({
                "é£Ÿè­œ1_ID": id1,
                "é£Ÿè­œ1_åç¨±": name1,
                "é£Ÿè­œ2_ID": id2,
                "é£Ÿè­œ2_åç¨±": name2,
                "ç›¸ä¼¼åº¦": round(sim_score, 3)
            })

    # æŒ‰ç›¸ä¼¼åº¦æ’åºï¼ˆå¾é«˜åˆ°ä½ï¼‰
    all_similarities.sort(key=lambda x: x["ç›¸ä¼¼åº¦"], reverse=True)

    # åˆ†æä¸åŒé–€æª»ä¸‹çš„çµæœ
    thresholds = [0.1, 0.2, 0.3, 0.4, 0.5]
    threshold_analysis = []
    for test_threshold in thresholds:
        count = sum(1 for sim in all_similarities if sim['ç›¸ä¼¼åº¦'] > test_threshold)
        threshold_analysis.append({
            "é–€æª»": test_threshold,
            "ç›¸ä¼¼é£Ÿè­œçµ„æ•¸": count
        })

    # è¨ˆç®—å»ºè­°é–€æª»å’Œçµ±è¨ˆè³‡è¨Š
    analysis_summary = {}
    if all_similarities:
        max_sim = all_similarities[0]['ç›¸ä¼¼åº¦']
        suggested_threshold = max(0.1, max_sim * 0.8)  # æœ€é«˜ç›¸ä¼¼åº¦çš„ 80%
        analysis_summary = {
            "æœ€é«˜ç›¸ä¼¼åº¦": max_sim,
            "å»ºè­°é–€æª»": round(suggested_threshold, 2),
            "ç¸½é£Ÿè­œçµ„åˆæ•¸": len(all_similarities),
            "åˆ†æé–€æª»": threshold,
            "ç¬¦åˆé–€æª»çš„çµ„åˆæ•¸": len([sim for sim in all_similarities if sim['ç›¸ä¼¼åº¦'] > threshold])
        }

    # é¡¯ç¤ºé«˜æ–¼ç•¶å‰é–€æª»çš„çµæœ
    similarity_results = [sim for sim in all_similarities if sim['ç›¸ä¼¼åº¦'] > threshold]

    # å„²å­˜çµæœåˆ° CSVï¼ˆä¿æŒåŸæœ¬æ ¼å¼ï¼‰
    if similarity_results:
        result_df = pd.DataFrame(similarity_results)
        # æŒ‰ç›¸ä¼¼åº¦æ’åºï¼ˆå¾é«˜åˆ°ä½ï¼‰
        result_df = result_df.sort_values('ç›¸ä¼¼åº¦', ascending=False)

        # åœ¨ CSV æª”æ¡ˆä¸­æ·»åŠ åˆ†æè³‡è¨Šä½œç‚ºè¨»è§£è¡Œ
        with open(output_csv_path, 'w', encoding='utf-8-sig', newline='') as f:
            # å¯«å…¥åˆ†æè³‡è¨Šä½œç‚ºè¨»è§£
            if analysis_summary:
                f.write(f"# åˆ†ææ‘˜è¦\n")
                f.write(f"# æœ€é«˜ç›¸ä¼¼åº¦: {analysis_summary['æœ€é«˜ç›¸ä¼¼åº¦']:.3f}\n")
                f.write(f"# å»ºè­°é–€æª»: {analysis_summary['å»ºè­°é–€æª»']:.2f} (æœ€é«˜ç›¸ä¼¼åº¦çš„80%)\n")
                f.write(f"# ç¸½é£Ÿè­œçµ„åˆæ•¸: {analysis_summary['ç¸½é£Ÿè­œçµ„åˆæ•¸']}\n")
                f.write(f"# ç•¶å‰åˆ†æé–€æª»: {analysis_summary['åˆ†æé–€æª»']}\n")
                f.write(f"# ç¬¦åˆé–€æª»çµ„åˆæ•¸: {analysis_summary['ç¬¦åˆé–€æª»çš„çµ„åˆæ•¸']}\n")
                f.write(f"#\n")

            # å¯«å…¥é–€æª»åˆ†æ
            f.write(f"# ä¸åŒé–€æª»ä¸‹çš„ç›¸ä¼¼é£Ÿè­œæ•¸é‡\n")
            for item in threshold_analysis:
                f.write(f"# é–€æª» {item['é–€æª»']}: {item['ç›¸ä¼¼é£Ÿè­œçµ„æ•¸']} çµ„ç›¸ä¼¼é£Ÿè­œ\n")
            f.write(f"#\n")
            f.write(f"# ç›¸ä¼¼åº¦çµæœ (é«˜æ–¼ {threshold})\n")

            # å¯«å…¥æ­£å¸¸çš„ CSV è³‡æ–™
            result_df.to_csv(f, index=False)

        print(f"âœ… æ‰¾åˆ° {len(similarity_results)} çµ„ç›¸ä¼¼é£Ÿè­œï¼Œçµæœå·²å„²å­˜åˆ°ï¼š{output_csv_path}")
    else:
        # å³ä½¿æ²’æœ‰ç›¸ä¼¼é£Ÿè­œï¼Œä¹Ÿå»ºç«‹åŒ…å«åˆ†æè³‡è¨Šçš„ CSV æª”æ¡ˆ
        with open(output_csv_path, 'w', encoding='utf-8-sig', newline='') as f:
            # å¯«å…¥åˆ†æè³‡è¨Š
            if analysis_summary:
                f.write(f"# åˆ†ææ‘˜è¦\n")
                f.write(f"# æœ€é«˜ç›¸ä¼¼åº¦: {analysis_summary['æœ€é«˜ç›¸ä¼¼åº¦']:.3f}\n")
                f.write(f"# å»ºè­°é–€æª»: {analysis_summary['å»ºè­°é–€æª»']:.2f} (æœ€é«˜ç›¸ä¼¼åº¦çš„80%)\n")
                f.write(f"# ç¸½é£Ÿè­œçµ„åˆæ•¸: {analysis_summary['ç¸½é£Ÿè­œçµ„åˆæ•¸']}\n")
                f.write(f"# ç•¶å‰åˆ†æé–€æª»: {analysis_summary['åˆ†æé–€æª»']}\n")
                f.write(f"# ç¬¦åˆé–€æª»çµ„åˆæ•¸: {analysis_summary['ç¬¦åˆé–€æª»çš„çµ„åˆæ•¸']}\n")
                f.write(f"#\n")

            # å¯«å…¥é–€æª»åˆ†æ
            f.write(f"# ä¸åŒé–€æª»ä¸‹çš„ç›¸ä¼¼é£Ÿè­œæ•¸é‡\n")
            for item in threshold_analysis:
                f.write(f"# é–€æª» {item['é–€æª»']}: {item['ç›¸ä¼¼é£Ÿè­œçµ„æ•¸']} çµ„ç›¸ä¼¼é£Ÿè­œ\n")
            f.write(f"#\n")
            f.write(f"# æ²’æœ‰æ‰¾åˆ°ç›¸ä¼¼åº¦é«˜æ–¼ {threshold} çš„é£Ÿè­œçµ„åˆ\n")

            # å¯«å…¥ç©ºçš„ CSV æ¨™é¡Œ
            empty_df = pd.DataFrame(columns=["é£Ÿè­œ1_ID", "é£Ÿè­œ1_åç¨±", "é£Ÿè­œ2_ID", "é£Ÿè­œ2_åç¨±", "ç›¸ä¼¼åº¦"])
            empty_df.to_csv(f, index=False)

        print(f"â„¹ï¸  æ²’æœ‰æ‰¾åˆ°ç›¸ä¼¼åº¦é«˜æ–¼ {threshold} çš„é£Ÿè­œçµ„åˆï¼Œå·²å»ºç«‹åŒ…å«åˆ†æè³‡è¨Šçš„çµæœæª”æ¡ˆï¼š{output_csv_path}")


def main():
    # å–å¾—è…³æœ¬æ‰€åœ¨ç›®éŒ„å’Œå°ˆæ¡ˆæ ¹ç›®éŒ„
    base_dir = os.path.dirname(os.path.abspath(__file__))
    project_root = os.path.abspath(os.path.join(base_dir, ".."))
    cleaned_csv_dir = os.path.join(project_root, "cleaned_csv")

    # è¨­å®šç›¸ä¼¼åº¦é–€æª»
    threshold = 0.3

    if not os.path.exists(cleaned_csv_dir):
        print(f"âŒ æ‰¾ä¸åˆ° cleaned_csv ç›®éŒ„ï¼š{cleaned_csv_dir}")
        sys.exit(1)

    # å°‹æ‰¾æ‰€æœ‰èœåè³‡æ–™å¤¾
    vegetable_dirs = [d for d in os.listdir(cleaned_csv_dir)
                     if os.path.isdir(os.path.join(cleaned_csv_dir, d))]

    if not vegetable_dirs:
        print("âŒ åœ¨ cleaned_csv ç›®éŒ„ä¸­æ‰¾ä¸åˆ°ä»»ä½•èœåè³‡æ–™å¤¾")
        sys.exit(1)

    print(f"ğŸ” æ‰¾åˆ° {len(vegetable_dirs)} å€‹èœåè³‡æ–™å¤¾ï¼š{', '.join(vegetable_dirs)}")
    print(f"ğŸ“ˆ ç›¸ä¼¼åº¦é–€æª»è¨­å®šç‚ºï¼š{threshold}")
    print("=" * 60)

    # è™•ç†æ¯å€‹èœåè³‡æ–™å¤¾
    for vegetable_name in vegetable_dirs:
        vegetable_dir = os.path.join(cleaned_csv_dir, vegetable_name)

        # è¨­å®šè¼¸å…¥å’Œè¼¸å‡ºæª”æ¡ˆè·¯å¾‘
        input_csv_path = os.path.join(vegetable_dir, f"{vegetable_name}_recipe_documents.csv")
        output_csv_path = os.path.join(vegetable_dir, f"{vegetable_name}_é£Ÿè­œç›¸ä¼¼åº¦åˆ†æ.csv")

        print(f"\nğŸ¥¬ è™•ç†èœåï¼š{vegetable_name}")

        if not os.path.exists(input_csv_path):
            print(f"âš ï¸  æ‰¾ä¸åˆ°æª”æ¡ˆï¼š{os.path.basename(input_csv_path)}ï¼Œè·³éæ­¤èœå")
            continue

        # åŸ·è¡Œç›¸ä¼¼åº¦åˆ†æ
        analyze_recipe_similarity(input_csv_path, output_csv_path, threshold)

    print("\nğŸ‰ æ‰€æœ‰èœåçš„ç›¸ä¼¼åº¦åˆ†æå®Œæˆï¼")


if __name__ == "__main__":
    main()

