import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import time
import random
import csv
import os
import sys

# è¨­å®š headers èˆ‡æ ¹è³‡æ–™å¤¾
headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
}
base_url = "https://icook.tw"
image_root = r"D:\AIPE\aipe_project\image"
raw_csv_root = r"D:\AIPE\aipe_project\raw_csv"
os.makedirs(image_root, exist_ok=True)
os.makedirs(raw_csv_root, exist_ok=True)

def load_vegetables_from_file(file_path: str) -> list:
    """å¾ vegetables.txt æª”æ¡ˆè®€å–è”¬èœé—œéµå­—æ¸…å–®"""
    vegetables = []
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in f:
                vegetable = line.strip()
                if vegetable:  # å¿½ç•¥ç©ºè¡Œ
                    vegetables.append(vegetable)
        print(f"ğŸ“‹ å¾ {file_path} è®€å–åˆ° {len(vegetables)} å€‹è”¬èœé—œéµå­—")
        return vegetables
    except FileNotFoundError:
        print(f"âŒ æ‰¾ä¸åˆ°æª”æ¡ˆï¼š{file_path}")
        return []
    except Exception as e:
        print(f"âŒ è®€å–æª”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        return []

def get_search_results(keyword: str, max_count: int = 20) -> list:
    results = []
    page = 1

    while len(results) < max_count:
        url = f"{base_url}/search/{keyword}?page={page}"
        print(f"ğŸ”„ è®€å–ç¬¬ {page} é ï¼š{url}")
        resp = requests.get(url, headers=headers)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.text, "html.parser")

        cards = soup.select("a.browse-recipe-link")
        if not cards:
            break

        for card in cards:
            recipe = {}
            recipe_name_tag = card.select_one("h2.browse-recipe-name")
            preview_ingredient_tag = card.select_one("p.browse-recipe-content-ingredient")
            article = card.select_one("article.browse-recipe-card")
            recipe_id = article.get("data-recipe-id") if article else ""
            img_tag = card.select_one("img.browse-recipe-cover-img")
            href = card.get("href")

            if recipe_name_tag and href and recipe_id:
                recipe["id"] = recipe_id
                recipe["name"] = recipe_name_tag.get_text(strip=True)
                recipe["url"] = urljoin(base_url, href)
                recipe["preview_ingredients"] = preview_ingredient_tag.get_text(strip=True).replace("é£Ÿæï¼š", "") if preview_ingredient_tag else ""
                img_url = img_tag.get("data-src") or img_tag.get("src") if img_tag else ""
                if img_url and "/w:200/" in img_url:
                    img_url = img_url.replace("/w:200/", "/")
                recipe["img_url"] = img_url
                if recipe["id"] not in [r["id"] for r in results]:
                    results.append(recipe)

            if len(results) >= max_count:
                break

        page += 1
        time.sleep(random.uniform(1, 1.5))

    return results

def get_recipe_details(recipe_url: str):
    resp = requests.get(recipe_url, headers=headers)
    resp.raise_for_status()
    soup = BeautifulSoup(resp.text, "html.parser")

    ingredients = []
    items = soup.select("li.ingredient")
    for li in items:
        name_tag = li.select_one(".ingredient-name")
        unit_tag = li.select_one(".ingredient-unit")
        name = name_tag.get_text(strip=True) if name_tag else ""
        unit = unit_tag.get_text(strip=True) if unit_tag else ""
        if name:
            ingredients.append(f"{name}{unit}")

    steps = []
    step_tags = soup.select("p.recipe-step-description-content")
    for step in step_tags:
        step_text = step.get_text(strip=True)
        if step_text:
            steps.append(step_text)

    return ingredients, steps

def download_image(url: str, path: str) -> str:
    if not url or url.startswith("data:image") or not url.startswith("http"):
        print(f"âš ï¸ è·³éç„¡æ•ˆåœ–ç‰‡ç¶²å€ï¼š{url}")
        return None

    try:
        resp = requests.get(url, headers=headers, timeout=10)
        resp.raise_for_status()
        with open(path, "wb") as f:
            f.write(resp.content)
        print(f"âœ… åœ–ç‰‡å·²ä¸‹è¼‰ï¼š{path}")
        return path
    except Exception as e:
        print(f"âš ï¸ ç„¡æ³•ä¸‹è¼‰åœ–ç‰‡ï¼š{url}ï¼ŒéŒ¯èª¤ï¼š{e}")
        return None

def save_to_csv(data_list: list, filename: str):
    with open(filename, "w", newline="", encoding="utf-8-sig") as f:
        writer = csv.writer(f, delimiter=";", quoting=csv.QUOTE_ALL)
        writer.writerow(["id", "é£Ÿè­œåç¨±", "ç¶²å€", "é è¦½é£Ÿæ", "è©³ç´°é£Ÿæ", "åšæ³•", "åœ–ç‰‡ç›¸å°è·¯å¾‘"])
        for item in data_list:
            writer.writerow([
                item["id"],
                item["name"],
                item["url"],
                item["preview_ingredients"],
                ", ".join(item["ingredients"]),
                " / ".join(item["steps"]),
                item.get("image_path", "")
            ])
    print(f"\nâœ… å·²å„²å­˜ {len(data_list)} ç­†è³‡æ–™åˆ° {filename}")

def main(keywords: list[str]):
    for keyword in keywords:
        print(f"\n============================\nğŸ” é—œéµå­—ï¼š{keyword}")
        image_folder = os.path.join(image_root, keyword)
        os.makedirs(image_folder, exist_ok=True)

        results = get_search_results(keyword, max_count=5)
        print(f"å…±å–å¾— {len(results)} ç­†é£Ÿè­œ")

        data_to_save = []

        for i, recipe in enumerate(results, 1):
            print(f"ğŸ½ ç¬¬ {i} ç­†ï¼š{recipe['name']}")
            filename_safe = recipe["name"].replace(" ", "_").replace("/", "_")
            image_filename = f"{recipe['id']}_{filename_safe}.jpg"
            image_path = os.path.join(image_folder, image_filename)
            rel_path = os.path.relpath(image_path, start=image_root)
            recipe["image_path"] = rel_path

            download_image(recipe["img_url"], image_path)

            try:
                ingredients, steps = get_recipe_details(recipe["url"])
                recipe["ingredients"] = ingredients
                recipe["steps"] = steps
            except Exception as e:
                print(f"âš ï¸ æŠ“è©³ç´°è³‡æ–™å¤±æ•—ï¼š{e}")
                recipe["ingredients"] = []
                recipe["steps"] = []

            data_to_save.append(recipe)
            print("-" * 40)
            time.sleep(random.uniform(1, 2))

        csv_filename = os.path.join(raw_csv_root, f"{keyword}_é£Ÿè­œè³‡æ–™.csv")
        save_to_csv(data_to_save, csv_filename)




if __name__ == "__main__":
    # å–å¾—è…³æœ¬æ‰€åœ¨ç›®éŒ„
    script_dir = os.path.dirname(os.path.abspath(__file__))
    vegetables_file = os.path.join(script_dir, "vegetables.txt")

    keywords = []

    # æª¢æŸ¥å‘½ä»¤åˆ—åƒæ•¸
    if len(sys.argv) > 1:
        # å¦‚æœæœ‰å‘½ä»¤åˆ—åƒæ•¸ï¼Œä½¿ç”¨æŒ‡å®šçš„è”¬èœåç¨±
        specified_vegetable = sys.argv[1].strip()
        keywords = [specified_vegetable]
        print(f"ğŸ¯ ä½¿ç”¨æŒ‡å®šçš„è”¬èœï¼š{specified_vegetable}")
    else:
        # æ²’æœ‰å‘½ä»¤åˆ—åƒæ•¸ï¼Œå¾ vegetables.txt è®€å–æ‰€æœ‰è”¬èœ
        keywords = load_vegetables_from_file(vegetables_file)
        if not keywords:
            print("âŒ ç„¡æ³•å¾ vegetables.txt è®€å–è”¬èœæ¸…å–®ï¼Œä½¿ç”¨é è¨­é—œéµå­—")
            keywords = ["å°ç™½èœ"]  # é è¨­é—œéµå­—
        else:
            print(f"ğŸ“ å°‡è™•ç† {len(keywords)} ç¨®è”¬èœï¼š{', '.join(keywords[:5])}{'...' if len(keywords) > 5 else ''}")

            # è©¢å•ä½¿ç”¨è€…æ˜¯å¦è¦è™•ç†æ‰€æœ‰è”¬èœ
            if len(keywords) > 1:
                choice = input(f"\næ˜¯å¦è¦çˆ¬å–æ‰€æœ‰ {len(keywords)} ç¨®è”¬èœçš„é£Ÿè­œï¼Ÿ(y/nï¼Œé è¨­ç‚º n)ï¼š").lower()
                if choice != 'y':
                    print("ğŸ›‘ å–æ¶ˆæ‰¹æ¬¡è™•ç†ï¼Œè«‹ä½¿ç”¨å‘½ä»¤åˆ—åƒæ•¸æŒ‡å®šå–®ä¸€è”¬èœ")
                    print("ä½¿ç”¨æ–¹æ³•ï¼špython crawl_icook_recipes.py <è”¬èœåç¨±>")
                    sys.exit(0)

    if keywords:
        print("=" * 50)
        print(f"ğŸš€ é–‹å§‹çˆ¬å–é£Ÿè­œï¼Œé—œéµå­—æ•¸é‡ï¼š{len(keywords)}")
        print("=" * 50)
        main(keywords)
    else:
        print("âŒ æ²’æœ‰å¯ç”¨çš„é—œéµå­—")
        sys.exit(1)
